{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proteinatlas.tsv.zip already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os # yes or no\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def download_files_from_hpa(url, max_size_gb=1, subfolder=\"downloaded_hpa_files\"):\n",
    "    # Create the subfolder if it doesn't exist\n",
    "    if not os.path.exists(subfolder):\n",
    "        os.makedirs(subfolder)\n",
    "\n",
    "    # Convert the max size from GB to bytes\n",
    "    max_size_bytes = max_size_gb * 1e9\n",
    "\n",
    "    # Make an HTTP GET request to the provided URL\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Ensure we got a successful response\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Search for all <a> tags with the specified href structure\n",
    "    links = soup.find_all('a', href=True)\n",
    "\n",
    "    # Base URL to prepend to relative file paths\n",
    "    base_url = \"https://www.proteinatlas.org\"\n",
    "\n",
    "    for link in links:\n",
    "        file_url = link['href']\n",
    "        if file_url.endswith('.zip'):  # Check if the link is to a .zip file\n",
    "            full_url = base_url + file_url\n",
    "\n",
    "            # Extract filename from the URL\n",
    "            filename = file_url.split('/')[-1]\n",
    "\n",
    "            # Create the full path to save the file\n",
    "            save_path = os.path.join(subfolder, filename)\n",
    "            \n",
    "            # Check if the file already exists\n",
    "            if os.path.exists(save_path):\n",
    "                print(f\"{filename} already exists. Skipping download.\")\n",
    "                continue\n",
    "\n",
    "            if filename == \"proteinatlas.tsv.zip\":\n",
    "                # Check file size without downloading the entire file\n",
    "                file_response = requests.head(full_url)\n",
    "                file_size = int(file_response.headers.get('Content-Length', 0))\n",
    "\n",
    "                if file_size <= max_size_bytes:\n",
    "                    # Download the file if it's within the size limit\n",
    "                    print(f\"Downloading {filename}...\")\n",
    "                    file_response = requests.get(full_url, stream=True)\n",
    "                    with open(save_path, 'wb') as file:\n",
    "                        for chunk in file_response.iter_content(chunk_size=8192):\n",
    "                            file.write(chunk)\n",
    "                    print(f\"{filename} downloaded!\")\n",
    "                else:\n",
    "                    print(f\"Skipping {filename} as it exceeds the size limit.\")\n",
    "\n",
    "# Example usage\n",
    "download_files_from_hpa(\"https://www.proteinatlas.org/about/download\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipped files to unzipped_folder\n",
      "Gene,\"Gene synonym\",Ensembl,\"Gene description\",Uniprot,Chromosome,Position,\"Protein class\",\"Biological process\",\"Molecular function\",\"Disease involvement\",Evidence,\"HPA evidence\",\"UniProt evidence\",\"NeXtProt evidence\",\"RNA tissue specificity\",\"RNA tissue distribution\",\"RNA tissue specificity score\",\"RNA tissue specific nTPM\",\"RNA single cell type specificity\",\"RNA single cell type distribution\",\"RNA single cell type specificity score\",\"RNA single cell type specific nTPM\",\"RNA cancer specificity\",\"RNA cancer distribution\",\"RNA cancer specificity score\",\"RNA cancer specific FPKM\",\"RNA brain regional specificity\",\"RNA brain regional distribution\",\"RNA brain regional specificity score\",\"RNA brain regional specific nTPM\",\"RNA blood cell specificity\",\"RNA blood cell distribution\",\"RNA blood cell specificity score\",\"RNA blood cell specific nTPM\",\"RNA blood lineage specificity\",\"RNA blood lineage distribution\",\"RNA blood lineage specificity score\",\"RNA blood lineage specific nTPM\",\"RNA cell line specificity\",\"RNA cell line distribution\",\"RNA cell line specificity score\",\"RNA cell line specific nTPM\",\"RNA tissue cell type enrichment\",\"RNA mouse brain regional specificity\",\"RNA mouse brain regional distribution\",\"RNA mouse brain regional specificity score\",\"RNA mouse brain regional specific nTPM\",\"RNA pig brain regional specificity\",\"RNA pig brain regional distribution\",\"RNA pig brain regional specificity score\",\"RNA pig brain regional specific nTPM\",Antibody,\"Reliability (IH)\",\"Reliability (Mouse Brain)\",\"Reliability (IF)\",\"Subcellular location\",\"Secretome location\",\"Secretome function\",\"CCD Protein\",\"CCD Transcript\",\"Blood concentration - Conc. blood IM [pg/L]\",\"Blood concentration - Conc. blood MS [pg/L]\",\"Blood expression cluster\",\"Tissue expression cluster\",\"Brain expression cluster\",\"Cell line expression cluster\",\"Single cell expression cluster\",Interactions,\"Subcellular main location\",\"Subcellular additional location\",\"Antibody RRID\",\"Pathology prognostics - Breast cancer\",\"Pathology prognostics - Cervical cancer\",\"Pathology prognostics - Colorectal cancer\",\"Pathology prognostics - Endometrial cancer\",\"Pathology prognostics - Glioma\",\"Pathology prognostics - Head and neck cancer\",\"Pathology prognostics - Liver cancer\",\"Pathology prognostics - Lung cancer\",\"Pathology prognostics - Melanoma\",\"Pathology prognostics - Ovarian cancer\",\"Pathology prognostics - Pancreatic cancer\",\"Pathology prognostics - Prostate cancer\",\"Pathology prognostics - Renal cancer\",\"Pathology prognostics - Stomach cancer\",\"Pathology prognostics - Testis cancer\",\"Pathology prognostics - Thyroid cancer\",\"Pathology prognostics - Urothelial cancer\"\n",
      "TSPAN6,\"T245, TM4SF6, TSPAN-6\",ENSG00000000003,\"Tetraspanin 6\",O43657,X,100627108-100639991,\"Predicted intracellular proteins, Predicted membrane proteins\",,,,\"Evidence at protein level\",\"Evidence at transcript level\",\"Evidence at protein level\",\"Evidence at protein level\",\"Low tissue specificity\",\"Detected in many\",,,\"Cell type enriched\",\"Detected in many\",6,\"Late spermatids: 1752.2\",\"Low cancer specificity\",\"Detected in all\",,,\"Low region specificity\",\"Detected in all\",,,\"Immune cell enhanced\",\"Detected in some\",,\"naive CD4 T-cell: 2.1\",\"Lineage enriched\",\"Detected in single\",7,\"T-cells: 2.1\",\"Low cancer specificity\",\"Detected in many\",,,\"Liver - Hepatocytes, Testis - Late spermatids, Thyroid - Thyroid glandular cells\",\"Low region specificity\",\"Detected in all\",,,\"Low region specificity\",\"Detected in all\",,,HPA004109,Approved,,Approved,\"Nucleoli fibrillar center,Cell Junctions,Cytosol\",,,NA,NA,,,\"Cluster 43: Non-specific - Transcription & Translation\",\"Cluster 8: Ciliated cells - Cilium organization\",\"Cluster 3: Choroid plexus - Cilium\",\"Cluster 53: Squamous epithelial cells - Keratinization\",\"Cluster 36: Late spermatids - Unknown function\",,\"Cell Junctions, Cytosol\",\"Nucleoli fibrillar center\",\"HPA004109: AB_1080301\",\"unprognostic (7.71e-2)\",\"unprognostic (8.97e-2)\",\"unprognostic (3.56e-2)\",\"unprognostic (2.57e-1)\",\"unprognostic (2.71e-3)\",\"unprognostic (5.93e-2)\",\"unprognostic (1.04e-1)\",\"unprognostic (1.09e-2)\",\"unprognostic (1.19e-2)\",\"unprognostic (1.80e-3)\",\"unprognostic (2.04e-3)\",\"unprognostic (8.76e-2)\",\"unprognostic (3.83e-3)\",\"unprognostic (4.28e-2)\",\"unprognostic (1.14e-1)\",\"unprognostic (2.24e-1)\",\"unprognostic (9.54e-3)\"\n",
      "TNMD,\"BRICD4, ChM1L, myodulin, TEM, tendin\",ENSG00000000005,Tenomodulin,Q9H2S6,X,100584936-100599885,\"Predicted membrane proteins\",,,,\"Evidence at protein level\",\"Evidence at transcript level\",\"Evidence at protein level\",\"Evidence at protein level\",\"Tissue enhanced\",\"Detected in some\",,\"adipose tissue: 20.4;breast: 12.5;seminal vesicle: 25.2\",\"Cell type enriched\",\"Detected in some\",6,\"Adipocytes: 72.2\",\"Low cancer specificity\",\"Detected in some\",,,\"Low region specificity\",\"Detected in some\",,,\"Not detected in immune cells\",\"Not detected\",,,\"Not detected\",\"Not detected\",,,\"Group enriched\",\"Detected in some\",5,\"testis cancer: 1.1;Uterine cancer: 1.3\",\"Skeletal muscle - Fibroblasts\",\"Low region specificity\",\"Detected in many\",,,\"Low region specificity\",\"Detected in many\",,,\"HPA034961, HPA055634\",Uncertain,,,,,,NA,NA,,,,\"Cluster 7: Adipose tissue - Mixed function\",\"Cluster 36: Choroid plexus - Mixed function\",\"Cluster 34: Testis cancer - Unknown function\",\"Cluster 41: Adipocytes & Endothelial cells - Mixed function\",1,,,\"HPA034961: AB_10670285, HPA055634: AB_2682868\",\"unprognostic (3.60e-2)\",,\"unprognostic (9.84e-3)\",\"unprognostic (3.19e-2)\",\"unprognostic (7.77e-3)\",,\"unprognostic (2.42e-1)\",,,\"unprognostic (2.15e-1)\",\"unprognostic (2.36e-2)\",\"unprognostic (1.37e-1)\",\"unprognostic (9.40e-2)\",\"unprognostic (9.60e-4)\",\"unprognostic (6.82e-2)\",\"unprognostic (5.67e-2)\",\"unprognostic (1.48e-1)\"\n",
      "DPM1,\"CDGIE, MPDS\",ENSG00000000419,\"Dolichyl-phosphate mannosyltransferase subunit 1, catalytic\",O60762,20,50934867-50959140,\"Disease related genes, Enzymes, Human disease related genes, Metabolic proteins, Plasma proteins, Potential drug targets, Predicted intracellular proteins\",,\"Glycosyltransferase, Transferase\",\"Congenital disorder of glycosylation, Congenital muscular dystrophy, Disease variant, Dystroglycanopathy\",\"Evidence at protein level\",\"Evidence at transcript level\",\"Evidence at protein level\",\"Evidence at protein level\",\"Low tissue specificity\",\"Detected in all\",,,\"Cell type enhanced\",\"Detected in all\",,\"Basal respiratory cells: 234.7;Syncytiotrophoblasts: 265.5\",\"Low cancer specificity\",\"Detected in all\",,,\"Low region specificity\",\"Detected in all\",,,\"Low immune cell specificity\",\"Detected in all\",,,\"Low lineage specificity\",\"Detected in all\",,,\"Low cancer specificity\",\"Detected in all\",,,,\"Low region specificity\",\"Detected in all\",,,\"Low region specificity\",\"Detected in all\",,,HPA051818,Approved,,,,,,NA,NA,,,\"Cluster 33: Non-specific - ATP binding\",\"Cluster 35: Non-specific - Mitochondria\",\"Cluster 21: Non-specific - Metabolism\",\"Cluster 62: Non-specific - Plasma proteins\",\"Cluster 35: Respiratory epithelial cells - Unknown function\",3,,,\"HPA051818: AB_2681624\",\"unprognostic (2.21e-2)\",\"unprognostic (4.94e-2)\",\"unprognostic (1.16e-1)\",\"unprognostic (4.24e-3)\",\"unprognostic (1.65e-1)\",\"unprognostic (3.27e-2)\",\"prognostic unfavorable (1.94e-6)\",\"unprognostic (1.10e-1)\",\"unprognostic (7.49e-2)\",\"unprognostic (2.53e-2)\",\"unprognostic (1.29e-2)\",\"unprognostic (3.61e-2)\",\"unprognostic (3.02e-3)\",\"unprognostic (4.98e-2)\",\"unprognostic (2.59e-1)\",\"unprognostic (3.58e-1)\",\"unprognostic (2.74e-1)\"\n",
      "SCYL3,\"PACE-1, PACE1\",ENSG00000000457,\"SCY1 like pseudokinase 3\",Q8IZE3,1,169849631-169894267,\"Enzymes, Predicted intracellular proteins\",,,,\"Evidence at protein level\",\"Evidence at protein level\",\"Evidence at protein level\",\"Evidence at protein level\",\"Low tissue specificity\",\"Detected in all\",,,\"Low cell type specificity\",\"Detected in many\",,,\"Low cancer specificity\",\"Detected in all\",,,\"Low region specificity\",\"Detected in all\",,,\"Low immune cell specificity\",\"Detected in all\",,,\"Low lineage specificity\",\"Detected in all\",,,\"Low cancer specificity\",\"Detected in all\",,,\"Liver - Hepatocytes\",\"Low region specificity\",\"Detected in all\",,,\"Low region specificity\",\"Detected in all\",,,\"HPA005624, HPA072383\",Approved,,Supported,\"Golgi apparatus,Cytosol\",,,NA,NA,,,\"Cluster 5: Non-specific - Cell proliferation\",\"Cluster 1: Liver & Kidney - Metabolism\",\"Cluster 50: Non-specific - Nucleic acid binding\",\"Cluster 45: Non-specific - Nuclear processes\",\"Cluster 55: Non-specific - Transcription\",4,\"Golgi apparatus, Cytosol\",,\"HPA005624: AB_1854916, HPA072383: \",\"unprognostic (2.35e-1)\",\"unprognostic (1.25e-1)\",\"unprognostic (7.53e-2)\",\"unprognostic (8.36e-2)\",\"unprognostic (7.13e-2)\",\"unprognostic (2.72e-3)\",\"unprognostic (1.31e-1)\",\"unprognostic (2.12e-2)\",\"unprognostic (2.85e-2)\",\"unprognostic (7.17e-2)\",\"unprognostic (8.80e-2)\",\"unprognostic (9.88e-2)\",\"unprognostic (1.20e-3)\",\"unprognostic (3.54e-1)\",\"unprognostic (1.75e-1)\",\"unprognostic (4.60e-2)\",\"prognostic favorable (8.85e-4)\"\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "# Function to unzip a file\n",
    "def unzip_file(zip_file_path, output_folder_path):\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(output_folder_path)\n",
    "        print(f\"Unzipped files to {output_folder_path}\")\n",
    "\n",
    "# Unzip the file\n",
    "unzip_file('downloaded_hpa_files/proteinatlas.tsv.zip', 'unzipped_folder')\n",
    "\n",
    "# Read the first 5 lines of the unzipped .tsv file\n",
    "try:\n",
    "    with open('unzipped_folder/proteinatlas.tsv', 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 5:\n",
    "                break\n",
    "            print(line.strip().replace('\\t', ','))\n",
    "except FileNotFoundError:\n",
    "    print(\"The file 'proteinatlas.tsv' was not found in the 'unzipped_folder'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded and saved as ontology/uhaf_marker_reference.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "def download_file_from_github(url, save_path, folder_name=\"ontology\"):\n",
    "    # Create folder if it doesn't exist\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    # Define the full path to save the file\n",
    "    full_save_path = os.path.join(folder_name, save_path)\n",
    "\n",
    "    # Download the file\n",
    "    response = requests.get(url)\n",
    "    with open(full_save_path, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "    print(f\"File downloaded and saved as {full_save_path}\")\n",
    "\n",
    "# URL to the hECA marker gene annotation file\n",
    "heca_url = \"https://github.com/XuegongLab/hECA/raw/main/UHAF/uHAF%20marker%20reference.xlsx\"\n",
    "\n",
    "# Name of the file to save\n",
    "heca_save_path = \"uhaf_marker_reference.xlsx\"\n",
    "\n",
    "# Download the file\n",
    "download_file_from_github(heca_url, heca_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n%pip install pandas openpyxl\\nfrom collections import defaultdict\\nimport pandas as pd\\nimport json\\n\\ndef convert_to_json(file_path, output_directory, is_hpa):\\n    # Create dictionaries for tissues/organs and cell types to markers\\n    tissue_to_marker = defaultdict(set)\\n    cell_type_to_marker = defaultdict(set)\\n\\n    if is_hpa:\\n        # Read the HPA Excel file\\n        df = pd.read_excel(file_path)\\n\\n        # Iterate through the DataFrame\\n        for index, row in df.iterrows():\\n            tissue = row[\\'Tissue\\']  # \\'Tissue\\' column used for HPA\\n            cell_type = row[\\'Cell type\\']\\n            marker = row[\\'Marker\\']\\n            \\n            tissue_to_marker[tissue.strip()].add(marker.strip())\\n            cell_type_to_marker[cell_type.strip()].add(marker.strip())\\n    else:\\n        # Read the hECA Excel file\\n        xls = pd.ExcelFile(file_path)\\n\\n        # Iterate over each sheet\\n        for sheet_name in xls.sheet_names:\\n            df = pd.read_excel(xls, sheet_name)\\n\\n            # Check for \\'Marker\\' or \\'marker\\' column\\n            marker_column = \\'Marker\\' if \\'Marker\\' in df.columns else \\'marker\\'\\n            if marker_column not in df.columns:\\n                raise KeyError(f\"Column \\'Marker\\' or \\'marker\\' not found in sheet {sheet_name}\")\\n\\n            # Iterate through the DataFrame\\n            for index, row in df.iterrows():\\n                tissue = sheet_name  # Use the sheet name as the tissue/organ for hECA\\n                cell_type = row[\\'cell_type\\']\\n                if pd.isnull(cell_type):\\n                    continue  # Skip rows where cell type is NaN or None\\n            cell_type = str(cell_type).strip()  # Convert to string and strip whitespace\\n        \\n            markers = set(map(str.strip, str(row[marker_column]).split(\\',\\')))  # Convert to string and split\\n\\n            tissue_to_marker[tissue.strip()].update(markers)\\n            cell_type_to_marker[cell_type].update(markers)\\n\\n    # Save to JSON files\\n    for category, cat_to_marker in [(\\'tissue\\', tissue_to_marker), (\\'cell_type\\', cell_type_to_marker)]:\\n        json_file_path = os.path.join(output_directory, f\"{\\'hpa\\' if is_hpa else \\'heca\\'}_{category}_to_marker.json\")\\n        with open(json_file_path, \\'w\\') as json_file:\\n            json.dump({k: list(v) for k, v in cat_to_marker.items()}, json_file, indent=4)\\n        print(f\"{\\'hpa\\' if is_hpa else \\'heca\\'} {category} to marker JSON saved at: {json_file_path}\")\\n\\n# Correct file paths before running the function\\nhpa_excel_file_path = \\'ontology/hpa_marker_reference.xlsx\\'\\nheca_excel_file_path = \\'ontology/uhaf_marker_reference.xlsx\\'\\noutput_directory = \\'ontology/json_output\\'\\n\\n# Make sure the output directory exists\\nos.makedirs(output_directory, exist_ok=True)\\n\\n# Convert and save the HPA Excel file to JSON\\nconvert_to_json(hpa_excel_file_path, output_directory, is_hpa=True)\\n\\n# Convert and save the hECA Excel file to JSON\\nconvert_to_json(heca_excel_file_path, output_directory, is_hpa=False)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "%pip install pandas openpyxl\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def convert_to_json(file_path, output_directory, is_hpa):\n",
    "    # Create dictionaries for tissues/organs and cell types to markers\n",
    "    tissue_to_marker = defaultdict(set)\n",
    "    cell_type_to_marker = defaultdict(set)\n",
    "\n",
    "    if is_hpa:\n",
    "        # Read the HPA Excel file\n",
    "        df = pd.read_excel(file_path)\n",
    "\n",
    "        # Iterate through the DataFrame\n",
    "        for index, row in df.iterrows():\n",
    "            tissue = row['Tissue']  # 'Tissue' column used for HPA\n",
    "            cell_type = row['Cell type']\n",
    "            marker = row['Marker']\n",
    "            \n",
    "            tissue_to_marker[tissue.strip()].add(marker.strip())\n",
    "            cell_type_to_marker[cell_type.strip()].add(marker.strip())\n",
    "    else:\n",
    "        # Read the hECA Excel file\n",
    "        xls = pd.ExcelFile(file_path)\n",
    "\n",
    "        # Iterate over each sheet\n",
    "        for sheet_name in xls.sheet_names:\n",
    "            df = pd.read_excel(xls, sheet_name)\n",
    "\n",
    "            # Check for 'Marker' or 'marker' column\n",
    "            marker_column = 'Marker' if 'Marker' in df.columns else 'marker'\n",
    "            if marker_column not in df.columns:\n",
    "                raise KeyError(f\"Column 'Marker' or 'marker' not found in sheet {sheet_name}\")\n",
    "\n",
    "            # Iterate through the DataFrame\n",
    "            for index, row in df.iterrows():\n",
    "                tissue = sheet_name  # Use the sheet name as the tissue/organ for hECA\n",
    "                cell_type = row['cell_type']\n",
    "                if pd.isnull(cell_type):\n",
    "                    continue  # Skip rows where cell type is NaN or None\n",
    "            cell_type = str(cell_type).strip()  # Convert to string and strip whitespace\n",
    "        \n",
    "            markers = set(map(str.strip, str(row[marker_column]).split(',')))  # Convert to string and split\n",
    "\n",
    "            tissue_to_marker[tissue.strip()].update(markers)\n",
    "            cell_type_to_marker[cell_type].update(markers)\n",
    "\n",
    "    # Save to JSON files\n",
    "    for category, cat_to_marker in [('tissue', tissue_to_marker), ('cell_type', cell_type_to_marker)]:\n",
    "        json_file_path = os.path.join(output_directory, f\"{'hpa' if is_hpa else 'heca'}_{category}_to_marker.json\")\n",
    "        with open(json_file_path, 'w') as json_file:\n",
    "            json.dump({k: list(v) for k, v in cat_to_marker.items()}, json_file, indent=4)\n",
    "        print(f\"{'hpa' if is_hpa else 'heca'} {category} to marker JSON saved at: {json_file_path}\")\n",
    "\n",
    "# Correct file paths before running the function\n",
    "hpa_excel_file_path = 'ontology/hpa_marker_reference.xlsx'\n",
    "heca_excel_file_path = 'ontology/uhaf_marker_reference.xlsx'\n",
    "output_directory = 'ontology/json_output'\n",
    "\n",
    "# Make sure the output directory exists\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Convert and save the HPA Excel file to JSON\n",
    "convert_to_json(hpa_excel_file_path, output_directory, is_hpa=True)\n",
    "\n",
    "# Convert and save the hECA Excel file to JSON\n",
    "convert_to_json(heca_excel_file_path, output_directory, is_hpa=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting openpyxl\n",
      "  Using cached openpyxl-3.1.2-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting numpy<2,>=1.26.0 (from pandas)\n",
      "  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Using cached et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m117.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, et-xmlfile, pandas, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 numpy-1.26.4 openpyxl-3.1.2 pandas-2.2.1 pytz-2024.1 tzdata-2024.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "hpa tissue to marker JSON saved at: ontology/json_output/hpa_tissue_to_marker.json\n",
      "hpa cell_type to marker JSON saved at: ontology/json_output/hpa_cell_type_to_marker.json\n",
      "heca tissue to marker JSON saved at: ontology/json_output/heca_tissue_to_marker.json\n",
      "heca cell_type to marker JSON saved at: ontology/json_output/heca_cell_type_to_marker.json\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas openpyxl\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "def convert_to_json(file_path, output_directory, is_hpa):\n",
    "    tissue_to_marker = defaultdict(set)\n",
    "    cell_type_to_marker = defaultdict(set)\n",
    "\n",
    "    if is_hpa:\n",
    "        df = pd.read_excel(file_path)\n",
    "        for _, row in df.iterrows():\n",
    "            tissue = row['Tissue']\n",
    "            cell_type = row['Cell type']\n",
    "            marker = row['Marker']\n",
    "            \n",
    "            tissue_to_marker[tissue.strip()].add(marker.strip())\n",
    "            cell_type_to_marker[cell_type.strip()].add(marker.strip())\n",
    "    else:\n",
    "        xls = pd.ExcelFile(file_path)\n",
    "        for sheet_name in xls.sheet_names:\n",
    "            df = pd.read_excel(xls, sheet_name)\n",
    "            marker_column = 'Marker' if 'Marker' in df.columns else 'marker'\n",
    "            for _, row in df.iterrows():\n",
    "                cell_type = row['cell_type'] if 'cell_type' in row else None\n",
    "                if pd.isnull(cell_type):\n",
    "                    continue\n",
    "                cell_type = str(cell_type).strip()\n",
    "        \n",
    "                markers = set(map(str.strip, str(row[marker_column]).split(',')))\n",
    "                tissue_to_marker[sheet_name.strip()].update(markers)\n",
    "                cell_type_to_marker[cell_type].update(markers)\n",
    "\n",
    "    for category, cat_to_marker in [('tissue', tissue_to_marker), ('cell_type', cell_type_to_marker)]:\n",
    "        json_file_path = os.path.join(output_directory, f\"{'hpa' if is_hpa else 'heca'}_{category}_to_marker.json\")\n",
    "        with open(json_file_path, 'w') as json_file:\n",
    "            json.dump({k: list(v) for k, v in cat_to_marker.items()}, json_file, indent=4)\n",
    "        print(f\"{'hpa' if is_hpa else 'heca'} {category} to marker JSON saved at: {json_file_path}\")\n",
    "\n",
    "output_directory = 'ontology/json_output'\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Convert and save the HPA Excel file to JSON\n",
    "hpa_excel_file_path = 'ontology/hpa_marker_reference.xlsx'\n",
    "convert_to_json(hpa_excel_file_path, output_directory, is_hpa=True)\n",
    "\n",
    "# Convert and save the hECA Excel file to JSON\n",
    "heca_excel_file_path = 'ontology/uhaf_marker_reference.xlsx'\n",
    "convert_to_json(heca_excel_file_path, output_directory, is_hpa=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'paneth cell', 't cell', 'endothelial cell', 'erythroid cell', 'cardiomyocyte cell', 'ductal cell', 'basal cell', 'dendritic cell', 'exocrine cell', 'fibroblast', 'proximal convoluted tubule', 'rod cell', 'smooth muscle cell', 'b cell', 'collecting duct', 'horizontal cell', 'urothelial cell', 'muller cell', 'distal convoluted tubule', 'goblet cell', 'sertoli cell', 'keratinocyte', 'cone cell', 'club cell/bronchiolar exocrine cell/clara cell'}\n",
      "\n",
      "\n",
      "t cell: {'common_markers': 1, 'percentage_common_markers_heca': 2.8, 'percentage_common_markers_hpa': 33.3}\n",
      "endothelial cell: {'common_markers': 4, 'percentage_common_markers_heca': 10.8, 'percentage_common_markers_hpa': 100.0}\n",
      "cardiomyocyte cell: {'common_markers': 3, 'percentage_common_markers_heca': 42.9, 'percentage_common_markers_hpa': 75.0}\n",
      "ductal cell: {'common_markers': 1, 'percentage_common_markers_heca': 25.0, 'percentage_common_markers_hpa': 33.3}\n",
      "fibroblast: {'common_markers': 2, 'percentage_common_markers_heca': 7.4, 'percentage_common_markers_hpa': 66.7}\n",
      "rod cell: {'common_markers': 2, 'percentage_common_markers_heca': 50.0, 'percentage_common_markers_hpa': 66.7}\n",
      "smooth muscle cell: {'common_markers': 2, 'percentage_common_markers_heca': 10.0, 'percentage_common_markers_hpa': 66.7}\n",
      "b cell: {'common_markers': 2, 'percentage_common_markers_heca': 25.0, 'percentage_common_markers_hpa': 66.7}\n",
      "horizontal cell: {'common_markers': 3, 'percentage_common_markers_heca': 100.0, 'percentage_common_markers_hpa': 100.0}\n",
      "muller cell: {'common_markers': 3, 'percentage_common_markers_heca': 75.0, 'percentage_common_markers_hpa': 100.0}\n",
      "goblet cell: {'common_markers': 2, 'percentage_common_markers_heca': 14.3, 'percentage_common_markers_hpa': 66.7}\n",
      "keratinocyte: {'common_markers': 3, 'percentage_common_markers_heca': 75.0, 'percentage_common_markers_hpa': 100.0}\n",
      "cone cell: {'common_markers': 2, 'percentage_common_markers_heca': 28.6, 'percentage_common_markers_hpa': 66.7}\n",
      "club cell/bronchiolar exocrine cell/clara cell: {'common_markers': 2, 'percentage_common_markers_heca': 22.2, 'percentage_common_markers_hpa': 66.7}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def create_custom_mapping():\n",
    "    \"\"\"\n",
    "    Define a mapping for cell type synonyms and specific exceptions.\n",
    "    This function now focuses on special cases not handled by automatic pluralization and normalization.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        # Explicit mappings for cases with missing words or different word ending\n",
    "        'bronchial epithelium, club cells': 'club cell/bronchiolar exocrine cell/clara cell',\n",
    "        'alveolar cells type 1': 'type I alveolar cell/type I pneumocyte',\n",
    "        'alveolar cells type 2': 'type II alveolar cell/type II pneumocyte',\n",
    "        'cone photoreceptor cells': 'cone cell',\n",
    "        'muller glia cells': 'muller cell',\n",
    "        'rod photoreceptor cells': 'rod cell',\n",
    "        'cardiomyocytes': 'cardiomyocyte cell',\n",
    "        'collecting duct cells': 'collecting duct',\n",
    "        'distal tubular cells': 'distal convoluted tubule',\n",
    "        'proximal tubular cells': 'proximal convoluted tubule',\n",
    "        'exocrine glandular cells': 'exocrine cell',\n",
    "        'basal glandular cells': 'basal cell',\n",
    "        'suprabasal keratinocytes': 'keratinocyte',\n",
    "        'spermatogonia ': 'differentiating spermatogonia',\n",
    "        'spermatogonia ': 'differentiated spermatogonia',\n",
    "        # Add or adjust mappings as needed for specific cases\n",
    "    }\n",
    "\n",
    "def normalize_name(name, custom_mapping):\n",
    "    \"\"\"\n",
    "    Normalize cell type names by applying custom mappings for known discrepancies, \n",
    "    including automatic adjustments for case, hyphens, underscores, \n",
    "    and handling singular/plural forms relevant to cell types.\n",
    "    \"\"\"\n",
    "    # Convert to lowercase and replace hyphens/underscores with spaces\n",
    "    normalized = name.strip().lower().replace(\"-\", \" \").replace(\"_\", \" \")\n",
    "\n",
    "    # Apply custom mappings first for specific synonyms and exceptions\n",
    "    normalized = custom_mapping.get(normalized, normalized)\n",
    "\n",
    "    # Automatically handle plural forms with basic English rules, tailored for cell types\n",
    "    if normalized.endswith('ies'):\n",
    "        normalized = re.sub('ies$', 'y', normalized)  # Correct rule for converting plurals ending in 'ies' to 'y'\n",
    "    elif normalized.endswith('es'):\n",
    "        # Correct handling for plurals ending in 'es', which might be common for certain biological terms\n",
    "        normalized = normalized[:-2]  # Removes the 'es', e.g., \"paneth cells\" to \"paneth cell\"\n",
    "    elif normalized.endswith('s') and not normalized.endswith('ss'):\n",
    "        normalized = normalized[:-1]  # General case for plurals not ending in 'ss', e.g., \"cells\" to \"cell\"\n",
    "\n",
    "    return normalized\n",
    "\n",
    "def load_and_normalize_data(file_path, custom_mapping):\n",
    "    \"\"\"\n",
    "    Load JSON data from a file, normalize cell type names using the custom mapping,\n",
    "    and return a dictionary with normalized cell type names as keys.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    # Normalize cell type names and return a new dictionary\n",
    "    return {normalize_name(key, custom_mapping): set(value) for key, value in data.items()}\n",
    "\n",
    "# Define your custom mapping\n",
    "custom_mapping = create_custom_mapping()\n",
    "\n",
    "# Load and normalize the data from JSON files\n",
    "heca_cell_type_to_marker = load_and_normalize_data('ontology/json_output/heca_cell_type_to_marker.json', custom_mapping)\n",
    "hpa_cell_type_to_marker = load_and_normalize_data('ontology/json_output/hpa_cell_type_to_marker.json', custom_mapping)\n",
    "\n",
    "common_cell_types = set(heca_cell_type_to_marker.keys()) & set(hpa_cell_type_to_marker.keys())\n",
    "print(common_cell_types)\n",
    "print('\\n')\n",
    "\n",
    "similarity_stats = {}\n",
    "\n",
    "for cell_type in common_cell_types:\n",
    "    heca_markers = heca_cell_type_to_marker[cell_type]\n",
    "    hpa_markers = hpa_cell_type_to_marker[cell_type]\n",
    "    common_markers = heca_markers & hpa_markers\n",
    "    \n",
    "    total_markers_heca = len(heca_markers)\n",
    "    total_markers_hpa = len(hpa_markers)\n",
    "    total_common_markers = len(common_markers)\n",
    "    \n",
    "    # Only add to similarity_stats if total_common_markers is greater than 0\n",
    "    if total_common_markers > 0:\n",
    "        similarity_stats[cell_type] = {\n",
    "            \"common_markers\": total_common_markers,\n",
    "            \"percentage_common_markers_heca\": round((total_common_markers / total_markers_heca) * 100, 1) if total_markers_heca else 0,\n",
    "            \"percentage_common_markers_hpa\": round((total_common_markers / total_markers_hpa) * 100, 1) if total_markers_hpa else 0,\n",
    "        }\n",
    "\n",
    "for cell_type, stats in similarity_stats.items():\n",
    "    print(f\"{cell_type}: {stats}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mygene\n",
      "  Using cached mygene-3.2.2-py2.py3-none-any.whl (5.4 kB)\n",
      "Collecting biothings-client>=0.2.6 (from mygene)\n",
      "  Using cached biothings_client-0.3.1-py2.py3-none-any.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: requests>=2.3.0 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from biothings-client>=0.2.6->mygene) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from requests>=2.3.0->biothings-client>=0.2.6->mygene) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from requests>=2.3.0->biothings-client>=0.2.6->mygene) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from requests>=2.3.0->biothings-client>=0.2.6->mygene) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from requests>=2.3.0->biothings-client>=0.2.6->mygene) (2023.11.17)\n",
      "Using cached biothings_client-0.3.1-py2.py3-none-any.whl (29 kB)\n",
      "Installing collected packages: biothings-client, mygene\n",
      "Successfully installed biothings-client-0.3.1 mygene-3.2.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1 input query terms found no hit:\t['ACPP']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion completed. Output saved to ontology/json_output/hpa_cell_type_to_ensembl.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8 input query terms found dup hits:\t[('IGHM', 2), ('TRBC1', 2), ('IGHG3', 2), ('TRBC2', 2), ('IGHG1', 2), ('IGHA1', 2), ('IGHA2', 2), ('\n",
      "25 input query terms found no hit:\t['CD11a', 'CD85g', 'NXRN1', 'CD41', 'CYP2F2', 'MTG', 'VIP1', 'SERPINBX', 'MT-CO1', 'CTGF', 'CD11c', \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion completed. Output saved to ontology/json_output/heca_cell_type_to_ensembl.json\n"
     ]
    }
   ],
   "source": [
    "%pip install mygene\n",
    "\n",
    "import mygene\n",
    "import json\n",
    "\n",
    "def convert_symbols_to_ensembl(input_json_path, output_json_path):\n",
    "    # Load gene symbols from the input JSON file\n",
    "    with open(input_json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Initialize MyGeneInfo\n",
    "    mg = mygene.MyGeneInfo()\n",
    "\n",
    "    # Prepare a list of all unique gene symbols across all cell types\n",
    "    gene_symbols = set()\n",
    "    for cell_type, markers in data.items():\n",
    "        gene_symbols.update(markers)\n",
    "\n",
    "    # Query MyGene.info to convert gene symbols to Ensembl IDs\n",
    "    query_results = mg.querymany(list(gene_symbols), scopes='symbol', fields='ensembl.gene', species='human')\n",
    "\n",
    "    # Create a mapping of gene symbols to Ensembl Gene IDs\n",
    "    symbol_to_ensembl = {}\n",
    "    for result in query_results:\n",
    "        if 'ensembl' in result:\n",
    "            if isinstance(result['ensembl'], list):  # Handling multiple Ensembl IDs for a gene symbol\n",
    "                symbol_to_ensembl[result['query']] = [ensembl['gene'] for ensembl in result['ensembl'] if 'gene' in ensembl]\n",
    "            else:\n",
    "                symbol_to_ensembl[result['query']] = result['ensembl']['gene']\n",
    "\n",
    "    # Convert the original data to use Ensembl Gene IDs\n",
    "    converted_data = {}\n",
    "    for cell_type, markers in data.items():\n",
    "        converted_markers = []\n",
    "        for marker in markers:\n",
    "            if marker in symbol_to_ensembl:\n",
    "                converted_markers.append(symbol_to_ensembl[marker])\n",
    "        converted_data[cell_type] = converted_markers\n",
    "\n",
    "    # Save the converted data to a new JSON file\n",
    "    with open(output_json_path, 'w') as outfile:\n",
    "        json.dump(converted_data, outfile, indent=4)\n",
    "\n",
    "    print(f\"Conversion completed. Output saved to {output_json_path}\")\n",
    "\n",
    "# Example usage:\n",
    "convert_symbols_to_ensembl('ontology/json_output/hpa_cell_type_to_marker.json', 'ontology/json_output/hpa_cell_type_to_ensembl.json')\n",
    "convert_symbols_to_ensembl('ontology/json_output/heca_cell_type_to_marker.json', 'ontology/json_output/heca_cell_type_to_ensembl.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (2.2.1)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Data written to output_files/cell_types_to_ensembl.json\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "%pip install pandas\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "class GeneExpressionAtlas:\n",
    "    def __init__(self, data_path, columns_to_check):\n",
    "        self.data_path = data_path\n",
    "        self.columns_to_check = columns_to_check\n",
    "        self.cell_types_to_ensembl = defaultdict(set)\n",
    "\n",
    "    def extract_cell_types(self):\n",
    "        df = pd.read_csv(self.data_path, sep='\\t')\n",
    "        for index, row in df.iterrows():\n",
    "            ensembl_id = row['Ensembl']\n",
    "            for col in self.columns_to_check:\n",
    "                cell_type_data = row[col]\n",
    "                if pd.notna(cell_type_data):\n",
    "                    for item in cell_type_data.split(';'):\n",
    "                        cell_type = item.split(':')[0].strip()\n",
    "                        self.cell_types_to_ensembl[cell_type].add(ensembl_id)\n",
    "        return self.cell_types_to_ensembl\n",
    "\n",
    "    def to_json(self, output_path):\n",
    "        # Convert sets to lists for JSON serialization\n",
    "        for cell_type, ensembl_ids in self.cell_types_to_ensembl.items():\n",
    "            self.cell_types_to_ensembl[cell_type] = list(ensembl_ids)\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(self.cell_types_to_ensembl, f)\n",
    "        return output_path\n",
    "\n",
    "class HPA(GeneExpressionAtlas):\n",
    "    def __init__(self, data_path):\n",
    "        columns_to_check = [\n",
    "            \"RNA tissue specific nTPM\",\n",
    "            \"RNA single cell type specific nTPM\",\n",
    "            \"RNA blood cell specific nTPM\",\n",
    "            \"RNA blood lineage specific nTPM\"\n",
    "        ]\n",
    "        super().__init__(data_path, columns_to_check)\n",
    "\n",
    "class hECA(GeneExpressionAtlas):\n",
    "    pass\n",
    "\n",
    "# Example usage for HPA\n",
    "hpa_data_path = \"unzipped_folder/proteinatlas.tsv\"\n",
    "hpa = HPA(hpa_data_path)\n",
    "\n",
    "# Extract cell types and associated Ensembl Gene IDs\n",
    "hpa_cell_types_to_ensembl = hpa.extract_cell_types()\n",
    "\n",
    "# Directory where the aggregated results will be saved\n",
    "output_directory = \"output_files\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Write the results to a JSON file\n",
    "json_file_path = hpa.to_json(f'{output_directory}/cell_types_to_ensembl.json')\n",
    "print(f\"Data written to {json_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (2.2.1)\n",
      "Requirement already satisfied: mygene in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (3.2.2)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: biothings-client>=0.2.6 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from mygene) (0.3.1)\n",
      "Requirement already satisfied: requests>=2.3.0 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from biothings-client>=0.2.6->mygene) (2.31.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from requests>=2.3.0->biothings-client>=0.2.6->mygene) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from requests>=2.3.0->biothings-client>=0.2.6->mygene) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from requests>=2.3.0->biothings-client>=0.2.6->mygene) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from requests>=2.3.0->biothings-client>=0.2.6->mygene) (2023.11.17)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "{'THBD', 'KRT5', 'AQP2', 'REG1A', 'CX3CR1', 'MYL7', 'APLNR', 'SCRG1', 'TMEM119', 'ELF3', 'OIT3', 'HLA-DPA1', 'CD99', 'LIPF', 'TM4SF4', 'CD19', 'KLRB1', 'IGLC2', 'FABP1', 'RAC2', 'KRT18', 'GNLY', 'TOP2A', 'THBS4', 'CAV2', 'SCGB1A1', 'FABP6', 'CHAD', 'ITLN1', 'UMOD', 'LDHB', 'TMEM213', 'SEMG1', 'LGALS2', 'CD11a', 'ADH1B', 'IGKC', 'VPREB1', 'CYSTM1', 'RPS27', 'IL32', 'CD85g', 'TTN', 'MUC5AC', 'HPX', 'LYVE1', 'HBA2', 'HLA-DRA', 'CLDN8', 'NCMAP', 'GATA2', 'COL3A1', 'TMEM174', 'TFF3', 'CD79A', 'CHST2', 'ARHGDIB', 'HBB', 'SELE', 'FCER1G', 'SLURP1', 'TUBA1B', 'NUPR1', 'CCL3L3', 'ACP3', 'KIR2DL4', 'IER2', 'ACTC1', 'CD3D', 'CYP17A1', 'IL1B', 'LGR5', 'PROX1', 'GC', 'SFTPA1', 'SLC6A5', 'STMN1', 'CPA2', 'AQP3', 'UCHL1', 'EMP2', 'CCDC80', 'MARCO', 'KRT15', 'MKI67', 'MTATP6P1', 'CLIC5', 'IGHM', 'ECSCR', 'SFTPC', 'SNTN', 'TM4SF1', 'CCL3', 'INS', 'EPCAM', 'CDH17', 'ASCL2', 'SPINK2', 'HAPLN1', 'SLC22A8', 'CXCL1', 'TRIM64', 'FCN1', 'VIM', 'WFDC21P', 'CPA4', 'CTSB', 'MUC2', 'PRKCG', 'ONECUT2', 'PMEL', 'DAPL1', 'S100A12', 'MT1X', 'FAM170B', 'VWF', 'NXRN1', 'CLEC3A', 'CD41', 'WT1', 'MUC6', 'CD79B', 'CYP2F2', 'IQCB1', 'CXCL14', 'BGN', 'COL15A1', 'CST7', 'RNF186', 'ABCC9', 'NR2E3', 'MTG', 'OPN1LW', 'RNASE3', 'MBP', 'CD163', 'PPEF2', 'EFNA1', 'CLC', 'LINC00682', 'VIP1', 'CAV1', 'CRYAB', 'SERPINBX', 'MYH11', 'SELENBP1', 'SLC12A1', 'AGER', 'CD5L', 'CLDN5', 'LEF1', 'KLK1', 'CHAC1', 'SPARC', 'CFD', 'CXCL3', 'LYZ', 'SPIB', 'PTPRC', 'S100A4', 'BMX', 'CELA3B', 'ONECUT1', 'OLIG1', 'MSLN', 'IGFBP7', 'CLU', 'CSTA', 'ERICH3', 'SLC34A2', 'KCNJ8', 'CST1', 'MYL2', 'LHX1', 'FNDC1', 'THEMIS', 'TRBC1', 'DEFA4', 'MUC1', 'XCL1', 'CDK1', 'TCL1A', 'COTL1', 'GRM4', 'CELA2A', 'CCNB1', 'RELN', 'SLCO6A1', 'FGF7', 'TRPM1', 'CXCL8', 'ACPP', 'S100B', 'CSH1', 'IGHG3', 'RARRES2', 'MS4A3', 'KLK3', 'AHSP', 'SFTPA2', 'NGFR', 'ACKR1', 'CLPS', 'HBG2', 'CD177', 'PLD4', 'GZMH', 'RGS1', 'HMGB2', 'STMN3', 'NOS2', 'COL9A1', 'UPK2', 'CELA3A', 'CNN1', 'COL1A2', 'IGF2', 'TPM1', 'VSIG4', 'NKG7', 'PDE6A', 'CCL21', 'PLP1', 'LYPD2', 'APOA1', 'RHO', 'CYTL1', 'CD3G', 'ACTA2', 'JCHAIN', 'CR2', 'TRBC2', 'MMRN1', 'ATP4B', 'BLVRB', 'CD93', 'GSN', 'ELANE', 'C1QL2', 'CD1C', 'CA7', 'ARR3', 'CD24', 'MT1H', 'TMSB15A', 'FSTL3', 'CD34', 'KRT7', 'PEG10', 'BEX2', 'POU5F1', 'XCL2', 'CHGB', 'MLANA', 'MS4A2', 'PF4', 'CPE', 'CCL18', 'DMBT1', 'NEUROD2', 'CD9', 'MRC1', 'GPAM', 'AMH', 'DES', 'PLAU', 'GATM', 'SEMA3D', 'PARM1', 'KRT6A', 'MT-CO1', 'TPT1', 'ATP6V1G3', 'UBE2C', 'AFP', 'C1QC', 'SLC12A3', 'GP9', 'DCT', 'IGHG1', 'NPPA', 'PLA2G2A', 'CRISP3', 'GAL', 'TYROBP', 'GUCA1B', 'PPY', 'PDPN', 'UTF1', 'SPDEF', 'FGG', 'CA1', 'CPB1', 'GABRG1', 'CTGF', 'SCARA5', 'CD74', 'FBLN1', 'APOE', 'CD11c', 'PODX1', 'HLA-DQB1', 'C1QA', 'COL2A1', 'FDCSP', 'AQP1', 'CLDN3', 'C8B', 'GUCA2A', 'RPL23A', 'TNNT2', 'FOXQ1', 'PRSS2', 'CST3', 'MUC17', 'KRT20', 'TSPAN8', 'CCL19', 'PVALB', 'FOS', 'BNC1', 'SAG', 'GPR183', 'APOC1', 'CCDC78', 'SCGB3A1', 'RPS18', 'RIMBP2', 'CCL2', 'CD13', 'TPSAB1', 'GAD1', 'CLCA1', 'TMSB4X', 'ID1', 'INSL3', 'IFNG', 'TTR', 'GAS2', 'CDH5', 'IGF1', 'GZMB', 'STMN2', 'PRR4', 'HSPB1', 'LCK', 'CHIA', 'IL1A', 'LGALS1', 'BCAM', 'SPINK4', 'DCN', 'RHEX', 'GLUL', 'HLA', 'GAS5', 'FSCN1', 'IL3RA', 'SCN7A', 'SCGB3A2', 'CD4', 'IGHA1', 'KRTDAP', 'LIPA', 'CD2000R1L', 'CFAP53', 'HBG1', 'AIF1', 'ANXA1', 'CFAP161', 'CD2', 'APOC3', 'PDGFRA', 'CCKAR', 'MZB1', 'TEX101', 'CD27', 'LEP', 'SYCP3', 'AGR2', 'MT-CO3', 'GJA4', 'TXN', 'DIO2', 'KRT17', 'CAMK2A', 'CCL1', 'LUM', 'LGR6', 'POSTN', 'MYL4', 'HLA-G', 'FOXJ1', 'TFF1', 'SPATA3', 'CD35', 'FEZF2', 'APOD', 'MAFB', 'S100A2', 'MIOX', 'PDGFRB', 'HLA-DRB1', 'MS4A12', 'SULT1E1', 'KISS1', 'KRT14', 'SPARCL1', 'HLA-DQA1', 'MT2A', 'IAPP', 'ALPI', 'RPLP2', 'GNAT2', 'TYRP1', 'NPY', 'MMP9', 'FABP4', 'GZMK', 'ELAVL4', 'PAGE4', 'SLC4A4', 'IGHA2', 'SNCG', 'FASN', 'MPO', 'PTK2B', 'ALB', 'AVP', 'RN7SK', 'CD3E', 'MGP', 'FTL', 'OPN1MW', 'CD36', 'IRF8', 'PRTN3', 'SCG5', 'CRYBB3', 'CLEC9A', 'NEFM', 'VCAN', 'DUSP2', 'RGS2', 'MS4A1', 'S100A8', 'GATD3A', 'SRGN', 'CD1a', 'ESAM', 'TMEM72', 'NXRN3', 'GAD2', 'DLK1', 'GJA5', 'FCGR3B', 'SPINK1', 'SBSN', 'SLPI', 'KNG1', 'HDC', 'DEFB1', 'OGN', 'GRM5', 'TNP1', 'UPK1A', 'GPRC5A', 'ACTG1', 'RAMP2', 'SCGB1D4', 'CAMK2B', 'SOX4', 'C7', 'SLC26A3', 'FLT1', 'DNAH12', 'ALDH1A1', 'COL10A1', 'CXCL12', 'CD69', 'DEFA5', 'EGR1', 'SST', 'CPA1', 'DEFB119', 'DNASE1L3', 'NR4A2', 'MMP7', 'SOCS7', 'RSPH1', 'PANTR1', 'IBSP', 'TUBB1', 'AQP8', 'CRP', 'RPSA', 'CD14', 'MYH7', 'NTS', 'LTF', 'TFF2', 'CCL5', 'AQP4', 'CPA3', 'ORM2', 'REG3A', 'RLBP1', 'PTTG1', 'IGHG4', 'SLC17A7', 'CYBRD1', 'SPO11', 'CD8A', 'AMY2A', 'HBA1', 'CPVL', 'CA4', 'RSPH4A', 'MYLK', 'BEST4', 'SLC26A4', 'GAP43', 'KRT10', 'FCER1A', 'MUC5B', 'GCG', 'C1QTNF3', 'UPK3B', 'PECAM1', 'IFI27', 'SLN', 'S100A9', 'GPX2', 'PLVAP', 'CD207', 'SELENOK', 'CALB1', 'SPACA1', 'HBEGF', 'MHCII', 'DST', 'LAMP3', 'ACTG2', 'BPIFB1', 'RGS5', 'LPO', 'PRM2', 'ALDOB', 'IL7R', 'IQCF3', 'CXXC1', 'HEMGN', 'NEFL', 'C11orf88', 'ACRV1', 'TAGLN', 'KRT13', 'IGFBP3', 'CCL4L2', 'CD15', 'FOXI1', 'KLK2', 'FCGR3A', 'LTB', 'AGR3', 'KRT19', 'KRT1', 'KRT8', 'KIT', 'TRAC', 'BPI', 'CD1A', 'GRM6', 'GFAP', 'FGA', 'EGR3', 'TPM2', 'PAGE2B', 'MAGEA8', 'CD53', 'MSR1', 'MND1', 'APBB1IP', 'OLIG2', 'CD52', 'CCL20', 'C1QB', 'MFAP5', 'CHGA', 'NAPSA', 'ORM1', 'CD44', 'CD123', 'MYL9', 'MT', 'TPSB2', 'KDR', 'REG1B', 'ALAS2', 'RETN', 'CFTR', 'PSG6', 'CEBPE', 'COL17A1', 'CLDN18', 'OVGP1', 'NOTCH3', 'CEACAM5', 'RBP4', 'PSG11', 'TG', 'NAT8', 'CYR61', 'SMIM24', 'GNG7', 'KRT4', 'S100P', 'UPK1B', 'FAM236C', 'DNAAF1', 'FBN1', 'SOX18', 'CD68', 'SLC2A3', 'RPS14', 'MSMB', 'SLC17A6', 'CCR7', 'IGLL1', 'SI', 'COL1A1'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8 input query terms found dup hits:\t[('IGHM', 2), ('TRBC1', 2), ('IGHG3', 2), ('TRBC2', 2), ('IGHG1', 2), ('IGHA1', 2), ('IGHA2', 2), ('\n",
      "26 input query terms found no hit:\t['CD11a', 'CD85g', 'NXRN1', 'CD41', 'CYP2F2', 'MTG', 'VIP1', 'SERPINBX', 'ACPP', 'MT-CO1', 'CTGF', '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed mapping gene symbols to Ensembl IDs.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas mygene\n",
    "import json\n",
    "import mygene\n",
    "import pandas as pd\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# Load marker genes from HPA and hECA\n",
    "hpa_markers = load_json('ontology/json_output/heca_cell_type_to_marker.json')\n",
    "heca_markers = load_json('ontology/json_output/hpa_cell_type_to_marker.json')\n",
    "\n",
    "# Combine all unique gene symbols from both sets\n",
    "all_genes = set()\n",
    "for genes in hpa_markers.values():\n",
    "    all_genes.update(genes)\n",
    "for genes in heca_markers.values():\n",
    "    all_genes.update(genes)\n",
    "\n",
    "print(all_genes)\n",
    "\n",
    "# Initialize mygene.MyGeneInfo\n",
    "mg = mygene.MyGeneInfo()\n",
    "\n",
    "# Query for Ensembl Gene IDs using gene symbols\n",
    "gene_info = mg.querymany(list(all_genes), scopes='symbol', fields='ensembl.gene', species='human')\n",
    "\n",
    "\n",
    "# Process query results to extract Ensembl Gene IDs, accommodating multiple IDs per gene symbols\n",
    "symbol_to_ensembl = {}\n",
    "for item in gene_info:\n",
    "    # Check for missing hits\n",
    "    if 'notfound' not in item:\n",
    "        # Initialize the list for this gene symbol if it's the first time we see it\n",
    "        if item['query'] not in symbol_to_ensembl:\n",
    "            symbol_to_ensembl[item['query']] = []\n",
    "        \n",
    "        # Handle cases where multiple Ensembl IDs are returned\n",
    "        if 'ensembl' in item:\n",
    "            if isinstance(item['ensembl'], list):\n",
    "                # Add all Ensembl IDs to the list\n",
    "                symbol_to_ensembl[item['query']].extend([gene['gene'] for gene in item['ensembl']])\n",
    "            else:\n",
    "                # Only one Ensembl ID, add it to the list\n",
    "                symbol_to_ensembl[item['query']].append(item['ensembl']['gene'])\n",
    "    else:\n",
    "        # Handle genes that were not found by adding an empty list\n",
    "        symbol_to_ensembl[item['query']] = []\n",
    "\n",
    "\n",
    "# Load protein atlas data\n",
    "protein_atlas_df = pd.read_csv('unzipped_folder/proteinatlas.tsv', sep='\\t')\n",
    "\n",
    "# Since some genes might not be found, filter out empty lists to avoid errors\n",
    "all_ensembl_ids = set(ensembl_id for ids_list in symbol_to_ensembl.values() for ensembl_id in ids_list if ids_list)\n",
    "\n",
    "\n",
    "# Filter rows where 'Ensembl' column matches any of the Ensembl IDs we found\n",
    "filtered_protein_atlas_df = protein_atlas_df[protein_atlas_df['Ensembl'].isin(all_ensembl_ids)]\n",
    "\n",
    "with open('ontology/json_output/gene_symbol_to_ensembl.json', 'w') as f:\n",
    "    json.dump(symbol_to_ensembl, f, indent=4)\n",
    "\n",
    "print(\"Completed mapping gene symbols to Ensembl IDs.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output_files/cell_types_for_all_genelists.json'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# Function to load a genelist from a file\n",
    "def load_genelist(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        # Remove quotes and strip whitespace from each line\n",
    "        return [line.strip().strip('\"') for line in f.read().splitlines()]\n",
    "\n",
    "# Load the cell_types_to_ensembl.json file\n",
    "cell_types_to_ensembl_filepath = 'output_files/cell_types_to_ensembl.json'\n",
    "with open(cell_types_to_ensembl_filepath, 'r') as f:\n",
    "    cell_types_to_ensembl = json.load(f)\n",
    "\n",
    "# Load all genelists using file paths for the gene lists\n",
    "genelists_files = [f'genelists/Genelist{i}.txt' for i in range(1, 7)]\n",
    "\n",
    "# Initialize a dictionary to store cell type frequencies for all genelists\n",
    "cell_types_for_all_genelists = {}\n",
    "\n",
    "# Loop through each genelist file\n",
    "for i, genelist_file in enumerate(genelists_files, 1):\n",
    "    # Load the current genelist\n",
    "    genelist = load_genelist(genelist_file)\n",
    "    \n",
    "    # Initialize a defaultdict to store the results for the current genelist\n",
    "    cell_types_for_genelist = defaultdict(int)\n",
    "\n",
    "    # Identify cell types associated with the genes in the current genelist\n",
    "    for cell_type, ensembl_ids in cell_types_to_ensembl.items():\n",
    "        for ensembl_id in genelist:\n",
    "            if ensembl_id in ensembl_ids:\n",
    "                cell_types_for_genelist[cell_type] += 1\n",
    "\n",
    "    # Store the results for the current genelist\n",
    "    cell_types_for_all_genelists[f'Genelist{i}'] = cell_types_for_genelist\n",
    "\n",
    "# Save the aggregated results to a JSON file\n",
    "cell_types_for_all_genelists_file = 'output_files/cell_types_for_all_genelists.json'\n",
    "with open(cell_types_for_all_genelists_file, 'w') as f:\n",
    "    json.dump(cell_types_for_all_genelists, f)\n",
    "\n",
    "cell_types_for_all_genelists_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ENSG00000000003',\n",
       "  'ENSG00000000005',\n",
       "  'ENSG00000000419',\n",
       "  'ENSG00000000457',\n",
       "  'ENSG00000000460'],\n",
       " 'output_files/protein_atlas_ensembl_ids.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the protein atlas data\n",
    "protein_atlas_filepath = 'unzipped_folder/proteinatlas.tsv'\n",
    "protein_atlas_df = pd.read_csv(protein_atlas_filepath, sep='\\t')\n",
    "\n",
    "# Extract the Ensembl IDs from the 'Gene' column\n",
    "ensembl_ids = protein_atlas_df['Ensembl'].unique().tolist()\n",
    "\n",
    "# Save the Ensembl IDs to a JSON file\n",
    "ensembl_ids_filepath = 'output_files/protein_atlas_ensembl_ids.json'\n",
    "with open(ensembl_ids_filepath, 'w') as f:\n",
    "    json.dump(ensembl_ids, f)\n",
    "\n",
    "# Check the first 5 Ensembl IDs\n",
    "ensembl_ids[:5], ensembl_ids_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scipy\n",
      "  Using cached scipy-1.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting statsmodels\n",
      "  Using cached statsmodels-0.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.5 kB)\n",
      "Collecting numpy<1.29.0,>=1.22.4 (from scipy)\n",
      "  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting pandas!=2.1.0,>=1.0 (from statsmodels)\n",
      "  Using cached pandas-2.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting patsy>=0.5.4 (from statsmodels)\n",
      "  Using cached patsy-0.5.6-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from statsmodels) (23.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pandas!=2.1.0,>=1.0->statsmodels) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas!=2.1.0,>=1.0->statsmodels)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas!=2.1.0,>=1.0->statsmodels)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from patsy>=0.5.4->statsmodels) (1.16.0)\n",
      "Using cached scipy-1.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.8 MB)\n",
      "Using cached statsmodels-0.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.7 MB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "Using cached pandas-2.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "Using cached patsy-0.5.6-py2.py3-none-any.whl (233 kB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, scipy, patsy, pandas, statsmodels\n",
      "Successfully installed numpy-1.26.4 pandas-2.2.1 patsy-0.5.6 pytz-2024.1 scipy-1.12.0 statsmodels-0.14.1 tzdata-2024.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'output_files/fisher_test_results.json'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install scipy statsmodels\n",
    "from scipy.stats import fisher_exact\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# Load all genelists using file paths for the gene lists\n",
    "genelist_filepaths = {f'Genelist{i}': f'genelists/Genelist{i}.txt' for i in range(1, 7)}\n",
    "\n",
    "# Paths to the input files\n",
    "cell_types_to_ensembl_filepath = 'output_files/cell_types_to_ensembl.json'\n",
    "protein_atlas_ensembl_ids_filepath = 'output_files/protein_atlas_ensembl_ids.json'\n",
    "\n",
    "# Load the Protein Atlas Ensembl IDs\n",
    "with open(protein_atlas_ensembl_ids_filepath, 'r') as f:\n",
    "    protein_atlas_ensembl_ids = set(json.load(f))\n",
    "\n",
    "# Load the mapping of cell types to Ensembl IDs\n",
    "with open(cell_types_to_ensembl_filepath, 'r') as f:\n",
    "    cell_types_to_ensembl = json.load(f)\n",
    "\n",
    "# Initialize a dictionary to store the results\n",
    "fisher_test_results = defaultdict(dict)\n",
    "\n",
    "# Perform Fisher's Exact Test for each genelist\n",
    "for genelist_name, genelist_filepath in genelist_filepaths.items():\n",
    "    # Load the genelist\n",
    "    with open(genelist_filepath, 'r') as f:\n",
    "        genelist = set(line.strip().strip('\"') for line in f.readlines())\n",
    "\n",
    "    # Total number of genes in the genelist and in the Protein Atlas\n",
    "    total_genes_genelist = len(genelist)\n",
    "    total_genes_atlas = len(protein_atlas_ensembl_ids)\n",
    "\n",
    "    p_values = []\n",
    "\n",
    "    # Perform the test for each cell type\n",
    "    for cell_type, ensembl_ids in cell_types_to_ensembl.items():\n",
    "        ensembl_ids_set = set(ensembl_ids)\n",
    "\n",
    "        # Count of genes in both the genelist and the cell type\n",
    "        count_in_both = len(genelist.intersection(ensembl_ids_set))\n",
    "        count_in_genelist_not_cell_type = len(genelist.difference(ensembl_ids_set))\n",
    "        count_in_cell_type_not_genelist = len(ensembl_ids_set.difference(genelist))\n",
    "        count_in_neither = total_genes_atlas - (count_in_both + count_in_genelist_not_cell_type + count_in_cell_type_not_genelist)\n",
    "\n",
    "        # Construct the contingency table\n",
    "        table = [\n",
    "            [count_in_both, count_in_cell_type_not_genelist],\n",
    "            [count_in_genelist_not_cell_type, count_in_neither]\n",
    "        ]\n",
    "\n",
    "        # Perform Fisher's Exact Test\n",
    "        odds_ratio, p_value = fisher_exact(table, alternative='greater')\n",
    "        p_values.append(p_value)\n",
    "\n",
    "        # Store the results without adjusted p-values first\n",
    "        fisher_test_results[genelist_name][cell_type] = {\n",
    "            'p_value': p_value,\n",
    "            'odds_ratio': odds_ratio,\n",
    "            'count_in_both': count_in_both,\n",
    "            'count_in_genelist_not_cell_type': count_in_genelist_not_cell_type,\n",
    "            'count_in_cell_type_not_genelist': count_in_cell_type_not_genelist,\n",
    "            'count_in_neither': count_in_neither\n",
    "        }\n",
    "\n",
    "    # Adjust p-values using the Benjamini-Hochberg procedure\n",
    "    _, pvals_corrected, _, _ = multipletests(p_values, alpha=0.05, method='fdr_bh')\n",
    "\n",
    "    # Store the adjusted p-values in the results\n",
    "    for (cell_type, _), adj_p_value in zip(fisher_test_results[genelist_name].items(), pvals_corrected):\n",
    "        fisher_test_results[genelist_name][cell_type]['adjusted_p_value'] = adj_p_value\n",
    "    \n",
    "    # Sort results by adjusted p_value in ascending order\n",
    "    fisher_test_results[genelist_name] = dict(sorted(fisher_test_results[genelist_name].items(), key=lambda x: x[1].get('adjusted_p_value', 1)))\n",
    "\n",
    "# Save the results to a JSON file\n",
    "results_filepath = 'output_files/fisher_test_results.json'\n",
    "with open(results_filepath, 'w') as f:\n",
    "    json.dump(fisher_test_results, f)\n",
    "\n",
    "results_filepath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mygene\n",
    "\n",
    "import mygene\n",
    "\n",
    "# Initialize mygene MyGeneInfo\n",
    "mg = mygene.MyGeneInfo()\n",
    "\n",
    "# Function to query MyGene.info and map genes to GO terms\n",
    "def get_go_terms(gene_list):\n",
    "    go_terms = mg.querymany(gene_list, scopes='ensembl.gene', fields='go', species='human')\n",
    "    return {result['query']: result.get('go', {}) for result in go_terms}\n",
    "\n",
    "# Load the fisher test results\n",
    "fisher_test_results_path = 'output_files/fisher_test_results.json'\n",
    "with open(fisher_test_results_path, 'r') as file:\n",
    "    fisher_test_results = json.load(file)\n",
    "\n",
    "# Combine all unique genes from all genelists to query for GO terms\n",
    "unique_genes = set()\n",
    "for genelist_filepath in genelist_filepaths.values():\n",
    "    with open(genelist_filepath, 'r') as file:\n",
    "        unique_genes.update(set(line.strip().strip('\"') for line in file.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: statsmodels in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (0.14.1)\n",
      "Requirement already satisfied: numpy<2,>=1.18 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from statsmodels) (1.26.3)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.4 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from statsmodels) (1.12.0)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.0 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from statsmodels) (2.2.0)\n",
      "Requirement already satisfied: patsy>=0.5.4 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from statsmodels) (0.5.6)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from statsmodels) (23.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pandas!=2.1.0,>=1.0->statsmodels) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pandas!=2.1.0,>=1.0->statsmodels) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pandas!=2.1.0,>=1.0->statsmodels) (2023.4)\n",
      "Requirement already satisfied: six in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from patsy>=0.5.4->statsmodels) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Genelist1': [{'cell_type': 'esophagus',\n",
       "   'p_value': 7.601491563693231e-83,\n",
       "   'odds_ratio': 38.03038138332256,\n",
       "   'count_in_both': 81,\n",
       "   'count_in_genelist_not_cell_type': 119,\n",
       "   'count_in_cell_type_not_genelist': 351,\n",
       "   'count_in_neither': 19611,\n",
       "   'adjusted_p_value': 1.0262013610985863e-80},\n",
       "  {'cell_type': 'Suprabasal keratinocytes',\n",
       "   'p_value': 4.524016635921081e-70,\n",
       "   'odds_ratio': 26.866237987563593,\n",
       "   'count_in_both': 78,\n",
       "   'count_in_genelist_not_cell_type': 122,\n",
       "   'count_in_cell_type_not_genelist': 464,\n",
       "   'count_in_neither': 19498,\n",
       "   'adjusted_p_value': 3.0537112292467296e-68},\n",
       "  {'cell_type': 'vagina',\n",
       "   'p_value': 1.6908530223420254e-62,\n",
       "   'odds_ratio': 65.7748344370861,\n",
       "   'count_in_both': 49,\n",
       "   'count_in_genelist_not_cell_type': 151,\n",
       "   'count_in_cell_type_not_genelist': 98,\n",
       "   'count_in_neither': 19864,\n",
       "   'adjusted_p_value': 7.608838600539114e-61},\n",
       "  {'cell_type': 'Basal keratinocytes',\n",
       "   'p_value': 7.369658263665469e-38,\n",
       "   'odds_ratio': 21.508183825208718,\n",
       "   'count_in_both': 43,\n",
       "   'count_in_genelist_not_cell_type': 157,\n",
       "   'count_in_cell_type_not_genelist': 251,\n",
       "   'count_in_neither': 19711,\n",
       "   'adjusted_p_value': 2.4872596639870958e-36},\n",
       "  {'cell_type': 'skin 1',\n",
       "   'p_value': 8.6332271896158e-28,\n",
       "   'odds_ratio': 10.425534896757918,\n",
       "   'count_in_both': 46,\n",
       "   'count_in_genelist_not_cell_type': 154,\n",
       "   'count_in_cell_type_not_genelist': 556,\n",
       "   'count_in_neither': 19406,\n",
       "   'adjusted_p_value': 2.330971341196266e-26}],\n",
       " 'Genelist2': [{'cell_type': 'Leydig cells',\n",
       "   'p_value': 2.1788500557207554e-12,\n",
       "   'odds_ratio': 7.860914148576415,\n",
       "   'count_in_both': 22,\n",
       "   'count_in_genelist_not_cell_type': 178,\n",
       "   'count_in_cell_type_not_genelist': 309,\n",
       "   'count_in_neither': 19653,\n",
       "   'adjusted_p_value': 2.9414475752230195e-10},\n",
       "  {'cell_type': 'Fibroblasts',\n",
       "   'p_value': 1.0161292677628894e-10,\n",
       "   'odds_ratio': 6.352030434397947,\n",
       "   'count_in_both': 22,\n",
       "   'count_in_genelist_not_cell_type': 178,\n",
       "   'count_in_cell_type_not_genelist': 381,\n",
       "   'count_in_neither': 19581,\n",
       "   'adjusted_p_value': 6.858872557399503e-09},\n",
       "  {'cell_type': 'Astrocytes',\n",
       "   'p_value': 1.9242371893637563e-09,\n",
       "   'odds_ratio': 3.5055032797323658,\n",
       "   'count_in_both': 37,\n",
       "   'count_in_genelist_not_cell_type': 163,\n",
       "   'count_in_cell_type_not_genelist': 1214,\n",
       "   'count_in_neither': 18748,\n",
       "   'adjusted_p_value': 8.659067352136903e-08},\n",
       "  {'cell_type': 'Peritubular cells',\n",
       "   'p_value': 1.2083477944302134e-08,\n",
       "   'odds_ratio': 5.573777118986659,\n",
       "   'count_in_both': 19,\n",
       "   'count_in_genelist_not_cell_type': 181,\n",
       "   'count_in_cell_type_not_genelist': 369,\n",
       "   'count_in_neither': 19593,\n",
       "   'adjusted_p_value': 4.0781738062019703e-07},\n",
       "  {'cell_type': 'Oligodendrocyte precursor cells',\n",
       "   'p_value': 1.848262349070478e-07,\n",
       "   'odds_ratio': 2.8124168671011485,\n",
       "   'count_in_both': 39,\n",
       "   'count_in_genelist_not_cell_type': 161,\n",
       "   'count_in_cell_type_not_genelist': 1583,\n",
       "   'count_in_neither': 18379,\n",
       "   'adjusted_p_value': 4.990308342490291e-06}],\n",
       " 'Genelist3': [],\n",
       " 'Genelist4': [],\n",
       " 'Genelist5': [],\n",
       " 'Genelist6': []}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install statsmodels\n",
    "import json\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Load Fisher test results\n",
    "results_filepath = 'output_files/fisher_test_results.json'\n",
    "with open(results_filepath, 'r') as file:\n",
    "    fisher_test_results = json.load(file)\n",
    "\n",
    "# Extract insights from the Fisher test results\n",
    "insights = {}\n",
    "p_value_threshold = 0.05  # Adjusted p-value threshold\n",
    "\n",
    "for genelist, cell_types in fisher_test_results.items():\n",
    "    associations = []\n",
    "    p_values = []\n",
    "    for cell_type, stats in cell_types.items():\n",
    "        p_value = stats.get('p_value', 1)\n",
    "        count_in_both = stats.get('count_in_both', 0)\n",
    "        count_in_genelist_not_cell_type = stats.get('count_in_genelist_not_cell_type', 0)\n",
    "        count_in_cell_type_not_genelist = stats.get('count_in_cell_type_not_genelist', 0)\n",
    "        count_in_neither = stats.get('count_in_neither', 0)\n",
    "\n",
    "        # Calculate the odds ratio\n",
    "        odds_ratio = (count_in_both * count_in_neither) / (count_in_genelist_not_cell_type * count_in_cell_type_not_genelist) if count_in_genelist_not_cell_type and count_in_cell_type_not_genelist else float('inf')\n",
    "\n",
    "        associations.append({\n",
    "            'cell_type': cell_type,\n",
    "            'p_value': p_value,\n",
    "            'odds_ratio': odds_ratio,  # Include the odds ratio\n",
    "            'count_in_both': count_in_both,\n",
    "            'count_in_genelist_not_cell_type': count_in_genelist_not_cell_type,\n",
    "            'count_in_cell_type_not_genelist': count_in_cell_type_not_genelist,\n",
    "            'count_in_neither': count_in_neither\n",
    "        })\n",
    "        p_values.append(p_value)\n",
    "\n",
    "    # Adjust p-values using the Benjamini-Hochberg procedure\n",
    "    reject, pvals_corrected, _, _ = multipletests(p_values, alpha=p_value_threshold, method='fdr_bh')\n",
    "\n",
    "    # Store only associations with an adjusted p-value below the threshold and less than 1.0\n",
    "    significant_associations = []\n",
    "    for association, adj_p_value, rej in zip(associations, pvals_corrected, reject):\n",
    "        if rej and adj_p_value < 1.0:\n",
    "            association['adjusted_p_value'] = adj_p_value\n",
    "            significant_associations.append(association)\n",
    "\n",
    "    # Sort the significant associations by adjusted p_value in ascending order\n",
    "    significant_associations.sort(key=lambda x: x.get('adjusted_p_value', 1))\n",
    "    insights[genelist] = significant_associations\n",
    "\n",
    "# Write the insights to a JSON file\n",
    "output_filepath = 'output_files/fisher_test_insights.json'\n",
    "with open(output_filepath, 'w') as f:\n",
    "    json.dump(insights, f)\n",
    "\n",
    "# Displaying the first few insights for review\n",
    "{key: insights[key][:5] for key in insights.keys()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting goatools\n",
      "  Using cached goatools-1.3.11-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting pandas (from goatools)\n",
      "  Using cached pandas-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting numpy (from goatools)\n",
      "  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting scipy (from goatools)\n",
      "  Using cached scipy-1.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting xlsxwriter (from goatools)\n",
      "  Downloading XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting statsmodels (from goatools)\n",
      "  Using cached statsmodels-0.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.5 kB)\n",
      "Collecting openpyxl (from goatools)\n",
      "  Using cached openpyxl-3.1.2-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting docopt (from goatools)\n",
      "  Using cached docopt-0.6.2-py2.py3-none-any.whl\n",
      "Collecting pydot (from goatools)\n",
      "  Using cached pydot-2.0.0-py3-none-any.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: requests in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from goatools) (2.31.0)\n",
      "Requirement already satisfied: rich in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from goatools) (13.7.0)\n",
      "Collecting et-xmlfile (from openpyxl->goatools)\n",
      "  Using cached et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from pandas->goatools) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->goatools)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->goatools)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting pyparsing>=3 (from pydot->goatools)\n",
      "  Using cached pyparsing-3.1.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from requests->goatools) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from requests->goatools) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from requests->goatools) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from requests->goatools) (2023.11.17)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from rich->goatools) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from rich->goatools) (2.17.2)\n",
      "Collecting patsy>=0.5.4 (from statsmodels->goatools)\n",
      "  Using cached patsy-0.5.6-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from statsmodels->goatools) (23.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->goatools) (0.1.2)\n",
      "Requirement already satisfied: six in /home/gitpod/.pyenv/versions/3.12.1/lib/python3.12/site-packages (from patsy>=0.5.4->statsmodels->goatools) (1.16.0)\n",
      "Using cached goatools-1.3.11-py3-none-any.whl (15.8 MB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "Using cached openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
      "Using cached pandas-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "Using cached pydot-2.0.0-py3-none-any.whl (22 kB)\n",
      "Using cached scipy-1.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.8 MB)\n",
      "Using cached statsmodels-0.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.7 MB)\n",
      "Downloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached patsy-0.5.6-py2.py3-none-any.whl (233 kB)\n",
      "Using cached pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Installing collected packages: pytz, docopt, xlsxwriter, tzdata, pyparsing, numpy, et-xmlfile, scipy, pydot, patsy, pandas, openpyxl, statsmodels, goatools\n",
      "Successfully installed docopt-0.6.2 et-xmlfile-1.1.0 goatools-1.3.11 numpy-1.26.4 openpyxl-3.1.2 pandas-2.2.0 patsy-0.5.6 pydot-2.0.0 pyparsing-3.1.1 pytz-2024.1 scipy-1.12.0 statsmodels-0.14.1 tzdata-2024.1 xlsxwriter-3.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install goatools\n",
    "import requests\n",
    "import gzip\n",
    "import os\n",
    "import urllib.request\n",
    "from collections import defaultdict\n",
    "import goatools\n",
    "from goatools.obo_parser import GODag\n",
    "from goatools.associations import read_ncbi_gene2go\n",
    "\n",
    "class GeneExpressionAtlas:\n",
    "    def __init__(self, data_path, columns_to_check, custom_mapping=None):\n",
    "        self.data_path = data_path\n",
    "        self.columns_to_check = columns_to_check\n",
    "        self.custom_mapping = custom_mapping if custom_mapping else self.create_custom_mapping()\n",
    "        self.cell_types_to_ensembl = defaultdict(set)\n",
    "        \n",
    "        # Specify paths for the ontology files\n",
    "        ontology_folder = \"ontology\"\n",
    "        self.go_obo_path = os.path.join(ontology_folder, \"go-basic.obo\")\n",
    "        self.gene2go_path = os.path.join(ontology_folder, \"gene2go\")\n",
    "        \n",
    "        # Download required files before loading them\n",
    "        self.download_and_prepare_ontology_files()\n",
    "\n",
    "        # Load the GO DAG and gene2go data\n",
    "        if os.path.exists(self.go_obo_path):\n",
    "            self.go_dag = GODag(self.go_obo_path)\n",
    "        if os.path.exists(self.gene2go_path):\n",
    "            self.gene2go = read_ncbi_gene2go(self.gene2go_path, taxids=[9606])  # Assuming human genes\n",
    "    \n",
    "    def create_custom_mapping(self):\n",
    "        \"\"\"\n",
    "        Define a mapping for cell type synonyms and specific exceptions.\n",
    "        This function now focuses on special cases not handled by automatic pluralization and normalization.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            # Explicit mappings for cases with missing words or different word ending\n",
    "            'bronchial epithelium, club cells': 'club cell/bronchiolar exocrine cell/clara cell',\n",
    "            'alveolar cells type 1': 'type I alveolar cell/type I pneumocyte',\n",
    "            'alveolar cells type 2': 'type II alveolar cell/type II pneumocyte',\n",
    "            'cone photoreceptor cells': 'cone cell',\n",
    "            'muller glia cells': 'muller cell',\n",
    "            'rod photoreceptor cells': 'rod cell',\n",
    "            'cardiomyocytes': 'cardiomyocyte cell',\n",
    "            'collecting duct cells': 'collecting duct',\n",
    "            'distal tubular cells': 'distal convoluted tubule',\n",
    "            'proximal tubular cells': 'proximal convoluted tubule',\n",
    "            'exocrine glandular cells': 'exocrine cell',\n",
    "            'basal glandular cells': 'basal cell',\n",
    "            'suprabasal keratinocytes': 'keratinocyte',\n",
    "            'spermatogonia ': 'differentiating spermatogonia',\n",
    "            'spermatogonia ': 'differentiated spermatogonia',\n",
    "            # Add or adjust mappings as needed for specific cases\n",
    "        }\n",
    "    \n",
    "    def map_symbols_to_ensembl(self, symbols):\n",
    "        \"\"\"Map a list of gene symbols to Ensembl IDs.\"\"\"\n",
    "        result = self.mg.querymany(symbols, scopes='symbol', fields='ensembl.gene', species='human')\n",
    "        return {item['query']: item['ensembl']['gene'] for item in result if 'ensembl' in item}\n",
    "\n",
    "    def fetch_go_terms(self, ensembl_ids):\n",
    "        \"\"\"Fetch GO terms for given Ensembl gene IDs using QuickGO API.\"\"\"\n",
    "        url = \"https://www.ebi.ac.uk/QuickGO/services/annotation/search\"\n",
    "        headers = {\"Accept\": \"application/json\"}\n",
    "        params = {\n",
    "            \"geneProductId\": \",\".join(ensembl_ids),\n",
    "            \"limit\": 100  # Adjust based on needs\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        if response.status_code == 200:\n",
    "            go_terms = response.json()['results']\n",
    "            return go_terms\n",
    "        else:\n",
    "            print(\"Failed to fetch GO terms\")\n",
    "            return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    def download_and_prepare_ontology_files(self):\n",
    "        go_obo_url = \"http://purl.obolibrary.org/obo/go/go-basic.obo\"\n",
    "        gene2go_url = \"ftp://ftp.ncbi.nlm.nih.gov/gene/DATA/gene2go.gz\"\n",
    "        # Download the files\n",
    "        self.download_file(go_obo_url, \"ontology\")\n",
    "        self.download_file(gene2go_url, \"ontology\", decompress=True)\n",
    "\n",
    "    def download_file(self, url, dest_folder, decompress=False):\n",
    "        if not os.path.exists(dest_folder):\n",
    "            os.makedirs(dest_folder)  # Create directory if it does not exist\n",
    "        filename = url.split('/')[-1]\n",
    "        dest_file_path = os.path.join(dest_folder, filename)\n",
    "\n",
    "        if url.startswith('ftp://'):\n",
    "            # Handle FTP download\n",
    "            urllib.request.urlretrieve(url, dest_file_path)\n",
    "            print(f\"Downloaded {filename} to {dest_file_path}\")\n",
    "        else:\n",
    "            # Handle HTTP/HTTPS download\n",
    "            with requests.get(url, stream=True) as response:\n",
    "                if response.status_code == 200:\n",
    "                    with open(dest_file_path, 'wb') as f:\n",
    "                        for chunk in response.iter_content(chunk_size=8192):\n",
    "                            f.write(chunk)\n",
    "                    print(f\"Downloaded {filename} to {dest_file_path}\")\n",
    "                else:\n",
    "                    print(f\"Error downloading {filename}: Status Code {response.status_code}\")\n",
    "\n",
    "        # Decompress if necessary\n",
    "        if decompress and filename.endswith('.gz'):\n",
    "            with gzip.open(dest_file_path, 'rb') as f_in:\n",
    "                with open(dest_file_path[:-3], 'wb') as f_out:  # Remove .gz extension\n",
    "                    f_out.write(f_in.read())\n",
    "            os.remove(dest_file_path)  # Remove the compressed file\n",
    "            print(f\"File decompressed to {dest_file_path[:-3]}\")\n",
    "\n",
    "# Example usage\n",
    "gene_expression_atlas = GeneExpressionAtlas(\n",
    "    data_path=\"unzipped_folder/proteinatlas.tsv\",\n",
    "    columns_to_check=[\"column1\", \"column2\"]\n",
    ")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
